{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20gg5ZuLSEiT"
      },
      "outputs": [],
      "source": [
        "def evaluation_ab(position, depth, alpha, beta):\n",
        "\n",
        "  if depth == 0 or len(legal_moves(position)) == 0:\n",
        "    return static_evluation(position)\n",
        "\n",
        "  if white_to_move(position) == True:\n",
        "    best_move_evaluation = -300 #300 is a commonly assigned value to the king by chess engines\n",
        "    for move in legal_moves(position).sort(move_order_heuristics):\n",
        "      node_evaluation = evaluation_ab(move, depth - 1, alpha, beta)\n",
        "      best_move_evaluation = max(best_move_evaluation, node_evaluation)\n",
        "      alpha = max(alpha, node_evaluation)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return best_move_evaluation\n",
        "\n",
        "  else:\n",
        "    best_move_evaluation_black = 300\n",
        "    for move in legal_moves(position).sort(move_order_heuristics):\n",
        "      node_evaluation = evaluation_ab(move, depth - 1, alpha, beta)\n",
        "      best_move_evaluation_black = min(best_move_evaluation_black, node_evaluation)\n",
        "      beta = min(beta, node_evaluation)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return best_move_evaluation_black \n",
        "\n",
        "def evaluation(current_position, depth): \n",
        "  return evaluation_ab(current_position, depth, -300, 300)\n",
        "\n",
        "def search(s, nnet):\n",
        "    if gameOver(s): return outcome(s)\n",
        "\n",
        "    if s not in visited:\n",
        "        visited.append(s)\n",
        "        P[s], v = nnet.predict(s)\n",
        "        return -v\n",
        "  \n",
        "    max_u, best_a = -float(\"inf\"), random.choice(legalMoves(s))\n",
        "    for a in legalMoves(s):\n",
        "        u = Q[s][a] + c*P[s][a]*sqrt(sum(N[s]))/(1+N[s][a])\n",
        "        if u > max_u:\n",
        "            max_u = u\n",
        "            best_a = a\n",
        "    a = best_a\n",
        "    \n",
        "    smove = makeMove(s, a)\n",
        "    v = search(smove, nnet)\n",
        "\n",
        "    Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a]+1)\n",
        "    global N[s][a]\n",
        "    N[s][a] += 1\n",
        "    return -v\n",
        "\n",
        "def policy(s, nnet):\n",
        "  for i in range(number_of_simulations):\n",
        "    search(s, nnet)\n",
        "  return [N[s][a] for a in legalMoves(s)]\n",
        "\n",
        "def data(nnet):\n",
        "  examples = []\n",
        "  examples_per_game = []\n",
        "  for i in range(number_of_selfplay_games):\n",
        "    game = startingPosition()\n",
        "    while True:\n",
        "      pi = policy(game, nnet)\n",
        "      examples_per_game.append((game, pi, 0))\n",
        "      game = makeMove(game, max(pi))\n",
        "      if gameOver(game):\n",
        "        for example in examples_per_game:\n",
        "          example[2] = outcome(game)\n",
        "        break\n",
        "    examples += examples_per_game\n",
        "  return examples\n",
        "\n",
        "def upgrade(nnet):\n",
        "  return nnet.train(data(nnet))\n",
        "\n",
        "def finalnet():\n",
        "  nnet = nnet.initialise()\n",
        "  for i in range(iterations):\n",
        "    new_nnet = upgrade(nnet)\n",
        "    win_ratio = pit(nnet, new_net, n = number_of_games)\n",
        "    if win_ratio > threshold:\n",
        "      nnet = new_nnet\n",
        "  return nnet\n",
        "\n",
        "# Python Project\n",
        "\n",
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import mean\n",
        "# Importing classification algorithms from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "def model_accuracy(x_train, x_test, y_train, y_test, classifier, grid = {}, \n",
        "                   iterations = 1):\n",
        "  # Initialising variable\n",
        "  scores = []\n",
        "  # Performing parameter grid search with 5-fold cross-validation a specified \n",
        "  # number of times; randomised search is performed for computationally heavy\n",
        "  # models\n",
        "  for i in range(iterations):\n",
        "    # Singling out computationally heavy models\n",
        "    if str(classifier) not in [\"SVC()\", \"KMeans()\"]:\n",
        "      # Balanced accuracy takes into account both sensitivity and specificity, and\n",
        "      # is hence a better accuracy measure than either of the two individually, or\n",
        "      # than raw accuracy, which can misevaluate models that give unbalanced results\n",
        "      grid_search = GridSearchCV(estimator = classifier, \n",
        "                                param_grid = grid,\n",
        "                                n_jobs=-1, \n",
        "                                cv = 5, \n",
        "                                scoring = \"balanced_accuracy\").fit(x_train, y_train)\n",
        "    else:\n",
        "      grid_search = RandomizedSearchCV(estimator = classifier,\n",
        "                                       param_distributions = grid,\n",
        "                                       n_jobs=-1,\n",
        "                                       cv = 5,\n",
        "                                       scoring = \"balanced_accuracy\").fit(x_train, y_train)\n",
        "    # Recording the best parameter combination and corresponding score for \n",
        "    # each iteration\n",
        "    scores.append([grid_search.best_score_, grid_search.best_estimator_])\n",
        "    # Finding the parameter combination which produces the best yield most often\n",
        "    best_params = max(set([i[1] for i in scores]), key=scores.count)\n",
        "    # Recording the yield of the best parameter combination\n",
        "    best_yield = mean([i[0] for i in scores if i[1] == best_params])\n",
        "  return [best_params, best_yield]\n",
        "\n",
        "def accuracies_per_dataset(data):\n",
        "  # Separating dependent variable from independent variables\n",
        "  x = data.iloc[:, :-1].values\n",
        "  y = data.iloc[:,-1].values\n",
        "  # Splitting the data into training data and test data\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "      x, y, test_size = 0.3, random_state = 0)\n",
        "  # Scaling the data\n",
        "  StandardScaler().fit_transform(x_train)\n",
        "  StandardScaler().fit_transform(x_test)\n",
        "\n",
        "  # Generating parameter dictionaries and iteration numbers for all the models\n",
        "  models = [[LogisticRegression(), \n",
        "            [{\"penalty\" : [\"none\", \"l2\"],\n",
        "              \"max_iter\" : [500]},\n",
        "              # Different solvers are required for diffent penalties\n",
        "              {\"penalty\" : [\"l1\", \"elasticnet\"],\n",
        "              \"solver\" : [\"saga\"],\n",
        "              \"l1_ratio\" : [0.2, 0.4, 0.6, 0.8],\n",
        "              # Increased maximum iterations to guarantee convergence\n",
        "              \"max_iter\" : [500]}]],\n",
        "            [MLPClassifier(),\n",
        "            {\"activation\" : [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
        "            \"solver\" : [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "            \"max_iter\" : [500]},\n",
        "            # Reduced number of iterations due to computational strain\n",
        "            5],\n",
        "            [KNeighborsClassifier(),\n",
        "            {\"n_neighbors\" : [i for i in range(1,40)]},\n",
        "            10],\n",
        "            # Searching over gammas for the \"poly\" kernel is far too\n",
        "            # computationally demanding\n",
        "            [SVC(),\n",
        "              [{\"C\" : [1, 10, 100],\n",
        "                \"kernel\" : [ \"rbf\", \"sigmoid\"],\n",
        "                \"gamma\" : [0.1, 1, 10]},\n",
        "              {\"C\" : [1, 10, 100], \n",
        "               \"kernel\" : [\"poly\"], \n",
        "               \"degree\" : [1, 5, 10]}]],\n",
        "            [DecisionTreeClassifier(),\n",
        "            {\"criterion\" : [\"gini\", \"entropy\"],\n",
        "            \"splitter\" : [\"best\", \"random\"],\n",
        "            \"min_samples_split\" : [i for i in range(2, 10)]},\n",
        "            10],\n",
        "            [RandomForestClassifier(),\n",
        "            {\"criterion\" : [\"gini\", \"entropy\"],\n",
        "              # I have run a few tests and found that simply increasing the \n",
        "              # number of estimators improves the yield more than tinkering with\n",
        "              # the other parameters\n",
        "            \"n_estimators\" : [500]},\n",
        "            10],\n",
        "            [KMeans(),\n",
        "            {\"n_clusters\" : [2],\n",
        "            \"n_init\" : [1, 5, 10, 15, 20]}],\n",
        "            [GaussianNB()]]\n",
        "\n",
        "  # Initialising list of accuracies per classification model\n",
        "  global accuracy_per_model\n",
        "  accuracy_per_model = []\n",
        "  # Computing the accuracy of each model\n",
        "  for model in models:\n",
        "    args = [x_train, x_test, y_train, y_test] + model\n",
        "    accuracy_per_model.append(model_accuracy(*args))\n",
        "  # Output list of model-model accuracy pairs\n",
        "  return accuracy_per_model\n",
        "\n",
        "# Reading the Diabetes dataset\n",
        "df0 = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Initialising a preliminarily data-cleaned version of the dataset\n",
        "dfnan = df0.copy()\n",
        "# Replacing zero values, which clearly represent missing values, with np.nan in \n",
        "# all columns other than \"Pregnancies\" and \"Outcome\"\n",
        "dfnan[dfnan.drop(['Pregnancies', 'Outcome'], axis = 1) == 0] = np.nan\n",
        "\n",
        "# Initialising the first fully data-cleaned version of the dataset\n",
        "df1 = dfnan.copy()\n",
        "# Selecting columns with np.nan\n",
        "ind = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "# Filling missing values with mean column values in the appropriate columns\n",
        "df1[ind] = df1[ind].fillna(df1.mean())\n",
        "\n",
        "# Creating the second fullydata-cleaned version of the dataset\n",
        "df2 = df1.drop(df1[np.any(\n",
        "    (df1[ind] < df1[ind].quantile(0.25) - \n",
        "     1.5*(df1[ind].quantile(0.75)-df1[ind].quantile(0.25))) \n",
        "    | (df1[ind] > df1[ind].quantile(0.75) +\n",
        "    1.5*(df1[ind].quantile(0.75)-df1[ind].quantile(0.25))), \n",
        "    1)].index)\n",
        "\n",
        "# Creating the third fully data-cleaned version of the dataset\n",
        "df3 = dfnan.drop(dfnan[np.any((np.isnan(dfnan)), 1)].index)\n",
        "\n",
        "# Creating the fourth fully data-cleaned version of the dataset\n",
        "df4 = df3.drop(df3[np.any(\n",
        "    (df3[ind] < df3[ind].quantile(0.25) - \n",
        "     1.5*(df3[ind].quantile(0.75)-df3[ind].quantile(0.25))) \n",
        "    | (df3[ind] > df3[ind].quantile(0.75) +\n",
        "    1.5*(df3[ind].quantile(0.75)-df3[ind].quantile(0.25))), \n",
        "    1)].index)\n",
        "\n",
        "# Initialising accuracy lists\n",
        "accuracy_of_datasets = []\n",
        "accuracies_by_dataset = []\n",
        "# Creating a list of thr datasets\n",
        "data = [df0, df1, df2, df3, df4]\n",
        "# Generating accuracy lists\n",
        "for i in data:\n",
        "  accuracies = accuracies_per_dataset(i)\n",
        "  # A list containing the accuracy per model per dataset\n",
        "  accuracies_by_dataset.append(accuracies)\n",
        "  best_model_score = max([i[1] for i in accuracy_per_model])\n",
        "  # A list containing the accuracy per dataset, determined by the accuracy of \n",
        "  # the best model for the dataset\n",
        "  accuracy_of_datasets.append(best_model_score)\n",
        "best_dataset_index = accuracy_of_datasets.index(max(accuracy_of_datasets))\n",
        "best_dataset_accuracies = accuracies_by_dataset[best_dataset_index]\n",
        "\n",
        "# Displaying model accuracy per dataset\n",
        "for i in range(len(data)):\n",
        "  if i == best_dataset_index:\n",
        "    caption = \"df%s (best dataset)\" %i\n",
        "  else:\n",
        "    caption = \"df%s\" %i\n",
        "  display(pd.DataFrame(accuracies_by_dataset[i],\n",
        "          columns = [\"Model\", \"Accuracy\"]).style.set_caption(caption))\n",
        "  \n",
        "# Preprocessing for the best dataset\n",
        "ds = data[best_dataset_index]\n",
        "x = ds.iloc[:, :-1].values\n",
        "y = ds.iloc[:,-1].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size = 0.3, random_state = 0)\n",
        "StandardScaler().fit_transform(x_train)\n",
        "StandardScaler().fit_transform(x_test)\n",
        "\n",
        "# Initialising dataset of various measures of accuracy for the best dataset\n",
        "df = pd.DataFrame(best_dataset_accuracies, columns = [\"Model\", \"Balanced accuracy\"])\n",
        "\n",
        "# Initialising variables\n",
        "confusion = []\n",
        "norm_confusion = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "accuracy = []\n",
        "\n",
        "# Recording the values of the different measures of accuracy\n",
        "for model in df[\"Model\"]:\n",
        "  y_pred = cross_val_predict(model, x_train, y_train, cv = 5)\n",
        "  tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
        "  confusion.append(confusion_matrix(y_train, y_pred))\n",
        "  norm_confusion.append(confusion_matrix(y_train, y_pred, \n",
        "                                         normalize = \"true\").round(2))\n",
        "  sensitivity.append(tp / (tp+fn))\n",
        "  specificity.append(tn / (tn+fp))\n",
        "  accuracy.append((tp+tn) / (tp+tn+fp+fn))\n",
        "\n",
        "# Adding the values to the dataset\n",
        "df[\"Confusion matrix\"] = confusion\n",
        "df[\"Normalised confusion matrix\"] = norm_confusion\n",
        "df[\"Sensitivity\"] = sensitivity\n",
        "df[\"Specificity\"] = specificity\n",
        "df[\"Raw accuracy\"] = accuracy\n",
        "\n",
        "# Displaying the result\n",
        "display(df.style.set_caption(\"df%s metadata\" % best_dataset_index))\n",
        "\n",
        "# Displaying the final model which maximises the balanced accuracy among the \n",
        "# datasets, classification algorithms, and parameter combinations tested\n",
        "final = df.loc[df[\"Balanced accuracy\"].idxmax()]\n",
        "print(\"Final model\")\n",
        "display(final[\"Model\"])\n",
        "\n",
        "# Applying the final model to the test data to get our final accuracy measures\n",
        "model = final[\"Model\"]\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "final[\"Balanced accuracy\"] = balanced_accuracy_score(y_test, y_pred)\n",
        "final[\"Confusion matrix\"] = confusion_matrix(y_test, y_pred)\n",
        "final[\"Normalised confusion matrix\"] = confusion_matrix(y_test, y_pred, \n",
        "                                         normalize = \"true\").round(2)\n",
        "final[\"Sensitivity\"] = tp / (tp+fn)\n",
        "final[\"Specificity\"] = tn / (tn+fp)\n",
        "final[\"Raw accuracy\"] = (tp+tn) / (tp+tn+fp+fn)\n",
        "display(final)"
      ]
    }
  ]
}