{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20gg5ZuLSEiT"
      },
      "outputs": [],
      "source": [
        "def evaluation_ab(position, depth, alpha, beta):\n",
        "\n",
        "  if depth == 0 or len(legal_moves(position)) == 0:\n",
        "    return static_evluation(position)\n",
        "\n",
        "  if white_to_move(position) == True:\n",
        "    best_move_evaluation = -300 #300 is a commonly assigned value to the king by chess engines\n",
        "    for move in legal_moves(position).sort(move_order_heuristics):\n",
        "      node_evaluation = evaluation_ab(move, depth - 1, alpha, beta)\n",
        "      best_move_evaluation = max(best_move_evaluation, node_evaluation)\n",
        "      alpha = max(alpha, node_evaluation)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return best_move_evaluation\n",
        "\n",
        "  else:\n",
        "    best_move_evaluation_black = 300\n",
        "    for move in legal_moves(position).sort(move_order_heuristics):\n",
        "      node_evaluation = evaluation_ab(move, depth - 1, alpha, beta)\n",
        "      best_move_evaluation_black = min(best_move_evaluation_black, node_evaluation)\n",
        "      beta = min(beta, node_evaluation)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return best_move_evaluation_black \n",
        "\n",
        "def evaluation(current_position, depth): \n",
        "  return evaluation_ab(current_position, depth, -300, 300)\n",
        "\n",
        "def search(s, nnet):\n",
        "    if gameOver(s): return outcome(s)\n",
        "\n",
        "    if s not in visited:\n",
        "        visited.append(s)\n",
        "        P[s], v = nnet.predict(s)\n",
        "        return -v\n",
        "  \n",
        "    max_u, best_a = -float(\"inf\"), random.choice(legalMoves(s))\n",
        "    for a in legalMoves(s):\n",
        "        u = Q[s][a] + c*P[s][a]*sqrt(sum(N[s]))/(1+N[s][a])\n",
        "        if u > max_u:\n",
        "            max_u = u\n",
        "            best_a = a\n",
        "    a = best_a\n",
        "    \n",
        "    smove = makeMove(s, a)\n",
        "    v = search(smove, nnet)\n",
        "\n",
        "    Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a]+1)\n",
        "    global N[s][a]\n",
        "    N[s][a] += 1\n",
        "    return -v\n",
        "\n",
        "def policy(s, nnet):\n",
        "  for i in range(number_of_simulations):\n",
        "    search(s, nnet)\n",
        "  return [N[s][a] for a in legalMoves(s)]\n",
        "\n",
        "def data(nnet):\n",
        "  examples = []\n",
        "  examples_per_game = []\n",
        "  for i in range(number_of_selfplay_games):\n",
        "    game = startingPosition()\n",
        "    while True:\n",
        "      pi = policy(game, nnet)\n",
        "      examples_per_game.append((game, pi, 0))\n",
        "      game = makeMove(game, max(pi))\n",
        "      if gameOver(game):\n",
        "        for example in examples_per_game:\n",
        "          example[2] = outcome(game)\n",
        "        break\n",
        "    examples += examples_per_game\n",
        "  return examples\n",
        "\n",
        "def upgrade(nnet):\n",
        "  return nnet.train(data(nnet))\n",
        "\n",
        "def finalnet():\n",
        "  nnet = nnet.initialise()\n",
        "  for i in range(iterations):\n",
        "    new_nnet = upgrade(nnet)\n",
        "    win_ratio = pit(nnet, new_net, n = number_of_games)\n",
        "    if win_ratio > threshold:\n",
        "      nnet = new_nnet\n",
        "  return nnet\n",
        "\n"
    }
  ]
}
